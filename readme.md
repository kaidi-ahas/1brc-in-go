# How fast can you aggregate huge amounts of data with minimal memory and CPU waste?

file with rows:
    station;temperature_float

1. Read the file
Line by line (streaming â€” not loading the whole file into memory).

2. map the station and all the temperatures

"station": [1.3, 2.7, 3.0, 4.3, 4.2]
map type:
key - string
value - []float64

3. Group by station name
Each station accumulates:
min
max
sum
count

For a slice of floats you must compute for each station:
1. minimum temperature
slices.Min()

2. maximum temperature
slices.Max()

3. sum
range over slice and build the sum

4. count
map can give a count

5. average temperature
avg = sum / count

Print the result to stdout in this format:

One line per station, like this:
{
Station=min/avg/max, 
Station=min/avg/max, 
...}

Like this:
{Hamburg=5.3/10.1/15.8, Berlin=3.2/7.9/12.4}